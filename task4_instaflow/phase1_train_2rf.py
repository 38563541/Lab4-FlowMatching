"""
Phase 1: Train 2-Rectified Flow Model from DDPM-Generated Data

This script trains a Flow Matching model on synthetic (x_0, x_1) pairs generated by the DDPM teacher.
The result is a 2-Rectified Flow model with straighter generation paths, which will serve as
the teacher for Phase 2 (InstaFlow distillation).

The training process is identical to standard Flow Matching, but uses synthetic data instead of real data.

Reference: Liu et al., "InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation"
"""

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path

import matplotlib
import matplotlib.pyplot as plt
import torch
from dotmap import DotMap
from pytorch_lightning import seed_everything
from tqdm import tqdm

sys.path.append('.')
from image_common.dataset import tensor_to_pil_image
from image_common.fm import FlowMatching, FMScheduler
from image_common.network import UNet

# Import reflow dataset from Task 3 (same format)
from task3_rectified_flow.reflow_dataset import ReflowDataset, get_reflow_data_iterator

matplotlib.use("Agg")


def get_current_time():
    now = datetime.now().strftime("%m-%d-%H%M%S")
    return now


def main(args):
    """config"""
    config = DotMap()
    config.update(vars(args))
    config.device = f"cuda:{args.gpu}"

    now = get_current_time()
    assert args.use_cfg, "In this assignment, we train with CFG setup only."

    # Create save directory
    save_dir = Path(f"results/2rf_from_ddpm-{now}")
    save_dir.mkdir(exist_ok=True, parents=True)
    print(f"save_dir: {save_dir}")

    seed_everything(config.seed)

    with open(save_dir / "config.json", "w") as f:
        json.dump(config, f, indent=2)

    # Load 2-RF training dataset (generated from DDPM in Phase 1)
    print(f"Loading 2-RF training dataset from {args.reflow_data_path}")
    reflow_dataset = ReflowDataset(
        args.reflow_data_path,
        use_cfg=args.use_cfg
    )

    train_dl = torch.utils.data.DataLoader(
        reflow_dataset,
        batch_size=config.batch_size,
        num_workers=4,
        shuffle=True,
        drop_last=True,
    )
    train_it = get_reflow_data_iterator(train_dl)

    # Get image resolution and num_classes from dataset metadata
    image_resolution = reflow_dataset.metadata.get("image_resolution", 64)
    num_classes = reflow_dataset.metadata.get("num_classes", None)

    print(f"Image resolution: {image_resolution}")
    if args.use_cfg:
        print(f"Number of classes: {num_classes}")

    # Set up the scheduler (same as base FM)
    fm_scheduler = FMScheduler(sigma_min=args.sigma_min)

    # Initialize network (same architecture as DDPM teacher)
    network = UNet(
        image_resolution=image_resolution,
        ch=128,
        ch_mult=[1, 2, 2, 2],
        attn=[1],
        num_res_blocks=4,
        dropout=0.1,
        use_cfg=args.use_cfg,
        cfg_dropout=args.cfg_dropout,
        num_classes=num_classes,
    )

    fm = FlowMatching(network, fm_scheduler)
    fm = fm.to(config.device)

    # Same optimizer and scheduler as base FM
    optimizer = torch.optim.Adam(fm.network.parameters(), lr=args.lr)
    scheduler = torch.optim.lr_scheduler.LambdaLR(
        optimizer, lr_lambda=lambda t: min((t + 1) / config.warmup_steps, 1.0)
    )

    step = 0
    losses = []
    print(f"Starting 2-Rectified Flow training for {config.train_num_steps} steps...")
    print("Goal: Learn straight paths from DDPM's guided outputs")

    with tqdm(initial=step, total=config.train_num_steps) as pbar:
        while step < config.train_num_steps:
            if step % config.log_interval == 0:
                fm.eval()
                # Plot loss curve
                plt.plot(losses)
                plt.xlabel('Training Step')
                plt.ylabel('Flow Matching Loss')
                plt.title('2-Rectified Flow Training Loss')
                plt.savefig(f"{save_dir}/loss.png")
                plt.close()

                # Generate sample images
                shape = (4, 3, fm.image_resolution, fm.image_resolution)
                if args.use_cfg:
                    class_label = torch.tensor([1, 1, 2, 3]).to(config.device)
                    samples = fm.sample(
                        shape,
                        class_label=class_label,
                        guidance_scale=7.5,
                        num_inference_timesteps=20,
                        verbose=False
                    )
                else:
                    samples = fm.sample(shape, num_inference_timesteps=20, verbose=False)

                pil_images = tensor_to_pil_image(samples)
                for i, img in enumerate(pil_images):
                    img.save(save_dir / f"step={step}-{i}.png")

                # Save checkpoint
                fm.save(f"{save_dir}/last.ckpt")
                fm.train()

            # Load batch from reflow dataset
            if args.use_cfg:
                x_0, x_1, label = next(train_it)
                x_0, x_1, label = x_0.to(config.device), x_1.to(config.device), label.to(config.device)
            else:
                x_0, x_1 = next(train_it)
                x_0, x_1 = x_0.to(config.device), x_1.to(config.device)
                label = None

            # Train with standard Flow Matching loss on synthetic pairs
            # This learns the straight path from x_0 to x_1 (DDPM's output)
            if args.use_cfg:
                loss = fm.get_loss(x_1, class_label=label, x0=x_0)
            else:
                loss = fm.get_loss(x_1, x0=x_0)

            pbar.set_description(f"Loss: {loss.item():.4f}")

            # Optimization step
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()
            losses.append(loss.item())

            step += 1
            pbar.update(1)

    print(f"Training completed! Final checkpoint saved at {save_dir}/last.ckpt")
    print("\n" + "="*80)
    print("Phase 1 complete! You now have a 2-Rectified Flow teacher model.")
    print(f"Next step: Generate distillation data for InstaFlow with:")
    print(f"python -m task4_instaflow.generate_instaflow_data \\")
    print(f"  --rf2_ckpt_path {save_dir}/last.ckpt \\")
    print(f"  --num_samples 50000 \\")
    print(f"  --save_dir data/afhq_instaflow \\")
    if args.use_cfg:
        print(f"  --use_cfg \\")
    print(f"  --cfg_scale 1.5")
    print("="*80)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Phase 1: Train 2-Rectified Flow from DDPM-generated data")
    parser.add_argument("--gpu", type=int, default=0)
    parser.add_argument("--reflow_data_path", type=str, required=True,
                        help="Path to 2-RF training dataset (generated by phase1_generate_2rf_data.py)")
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument(
        "--train_num_steps",
        type=int,
        default=100000,
        help="Number of training steps",
    )
    parser.add_argument("--warmup_steps", type=int, default=200)
    parser.add_argument("--log_interval", type=int, default=200)
    parser.add_argument("--lr", type=float, default=2e-4, help="Learning rate")
    parser.add_argument("--sigma_min", type=float, default=0.001)
    parser.add_argument("--seed", type=int, default=63)
    parser.add_argument("--use_cfg", action="store_true")
    parser.add_argument("--cfg_dropout", type=float, default=0.1)
    args = parser.parse_args()
    main(args)
